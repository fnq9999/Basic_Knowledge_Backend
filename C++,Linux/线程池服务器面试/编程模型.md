事件处理和并发
=====================
首先阻塞IO,信号IO,IO复用都是同步IO<br>
异步的意思是由内核来完成数据从内核去拷贝到用户区的过程
### 为什么采用  半同步/半反应堆+模拟proactor?
- 跑在多核机器上的话，多线程可以降低延时，提高响应速度
- proactor比reactor模式具有更高的并发度，但是Linux最新的异步IO方式：IO_uring还没去学。[Linux异步IO新时代：io_uring ](https://www.joyk.com/dig/detail/1562236294680133?page=17)
- 理论上 Proactor 比 Reactor 效率更高，异步 I/O 更加充分发挥 DMA(Direct Memory Access，直接内存存取)的优势。参考:[高性能网络编程中的线程模型看完这篇就会了！](https://zhuanlan.zhihu.com/p/137506808)
- proactor缺点
    - 1）编程复杂性，由于异步操作流程的事件的初始化和事件完成在时间和空间上都是相互分离的，因此开发异步应用程序更加复杂。应用程序还可能因为反向的流控而变得更加难以 Debug；
    - 2）内存使用，缓冲区在读或写操作的时间段内必须保持住，可能造成持续的不确定性，并且每个并发操作都要求有独立的缓存，相比 Reactor 模式，在 Socket 已经准备好读或写前，是不要求开辟缓存的；
    - 3）操作系统支持，Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下，Linux 2.6 才引入，目前异步 I/O 还不完善。

### 半同步，半反应堆+模拟proactor工作方式？
- 将reactor方式中，IO操作放到了工作线程当中
### 还知道哪些事件处理模式？
- Reactor 
    - 单reactor单线程<br>
        - <img src="https://pic1.zhimg.com/80/v2-2672df9ba7b92a273b780f06d5c7e900_1440w.jpg" width="30%"><br>
    - 优缺点
        - 模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成
        - 无法发挥多核优势，并且一旦一个业务卡住，那么整个流程都会卡住，延时增高，并发性低，一旦那个线程卡死，整个系统卡死
        - 客户端的数量有限，业务处理非常快速，比如 Redis，业务处理的时间复杂度 O(1)。？？？（这点还不是确定）

    - 单reactor多线程<br>
        - <img src="https://pic4.zhimg.com/80/v2-ccdc8fd652ceeb57010c46e6c7339ce7_1440w.jpg" width="30%"><br>
    - 优缺点
        - 利用多核优势
        - 多线程数据共享和访问比较复杂；Reactor 承担所有事件的监听和响应，在单线程中运行，高并发场景下容易成为性能瓶颈。
    - 主从reactor多线程<br>
        - <img src="https://pic2.zhimg.com/80/v2-a97daf1cca9aa97307b6cf125f387201_1440w.jpg" width="30%"><br>
    - 优点
        - 优点：父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型，Memcached 主从多线程，Netty 主从多线程模型的支持
- Proactor
    - 主线程event_loop事件并且向内核注册读写事件，工作线程负责逻辑处理
- 模拟Proactor
    - 主线程负责event_loop,和读写，工作线程进行逻辑处理
    
### 事件处理模式和并发模式区别
- 事件处理模式是上面那三个
- 并发模式
    - 半同步、半异步
    - 领导者、追随者

### 为什么用epoll?
- 优点
    - 用户态只遍历就绪事件
    - 内核中使用回调方式没有遍历
    - 线程安全，就算这个在修改注册事件，别的就算再epoll_wait也安全
    - 就算没人调用epoll_wait，在内核当中也会将事件放入rdllist,就比如ET模式
    - 用户超过1000，并且使用长连接。

### epoll 惊群问题

- 在2.6.18内核中accept的惊群问题已经被解决了，但是在epoll中仍然存在惊群问题，表现起来就是当多个进程/线程调用epoll_wait时会阻塞等待，当内核触发可读写事件，所有进程/线程都会进行响应，但是实际上只有一个进程/线程真实处理这些事件。
- 在epoll官方没有正式修复这个问题之前，Nginx作为知名使用者采用全局锁来限制每次可监听fd的进程数量，每次只有1个可监听的进程，后来在Linux 3.9内核中增加了SO_REUSEPORT选项实现了内核级的负载均衡，Nginx1.9.1版本支持了reuseport这个新特性，从而解决惊群问题。
- EPOLLEXCLUSIVE是在2016年Linux 4.5内核新添加的一个 epoll 的标识，Ngnix 在 1.11.3 之后添加了NGX_EXCLUSIVE_EVENT选项对该特性进行支持。EPOLLEXCLUSIVE标识会保证一个事件发生时候只有一个线程会被唤醒，以避免多侦听下的惊群问题。


### 惊群的解决办法
- 把fd只放入一个epoll当中（从根本上解决）
- 采用EPOLLEXCLUSIVE
- 用EPOLLONESHOT来决解决
- 采用socket的 REUSEPORT

### 线程间怎么异步唤醒？
参考：[线程间通信和异步唤醒](https://blog.csdn.net/daaikuaichuan/article/details/88808353)
- 不需要，但是需要的话可以使用eventfd实现用户态的线程间等待和通知（需要是因为有的进程还在 epoll_wait，需要唤醒）
    - 线程A调用write()向eventfd写入8个字节，线程B调用read()读取eventfd，如果读取到了8个字节，则线程B被唤醒或者说切换到线程B。
    - 好处：eventfd只使用了一个文件描述符且缓冲区长度只有8个字节，简洁高效。
- 要不就管道pipe

### epoll和select和poll对比
- select
- 见C++/linux文件夹下的文章
- 时间精确到微妙选择select，跨多个平台
- 连接数量少于1000，or大于1000但是都是短连接，or不支持Linux用poll
- 大于1000且相对来说都是长连接，则使用epoll


### 为什么采用线程池？
- 为什么是线程池而不是进程池，
- 为什么使用池化
    - 避免了频繁的创建和撤销
- 线程同步的方式
    - 互斥锁+条件变量，符合生产者消费者模型
    - 见C++/linux/面试题中文章
- 线程同步机制还有哪些？
    - 信号量
    - 读写锁
    - 自旋锁
    - 信号
- 线程处理完一个工作干嘛
    - 加锁，竞争队列中的内容
- 如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢？
    - 一个线程处理多个用户请求，并不是一个请求一个线程的模型，是多对1
    - 采用优秀的并发模型，例如真正的proactor
- 如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢?
    - 多核的情况下，只会吧这个客户的请求压力都放在一个核上，而不会影响别的线程
### 原理



